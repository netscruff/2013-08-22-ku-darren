{
 "metadata": {
  "name": "01-BeautfulSoup-II"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since we will need to parse multiple urls for results we should put the code we run repeatedly into a Python function definition"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib, urllib2\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "def page_results(soup):\n",
      "    \"\"\"Return a list of names\n",
      "    \n",
      "    Given an instance of BeautifulSoup, will return all the names it can find\n",
      "    in the search result table\n",
      "\n",
      "    \"\"\"\n",
      "    result_table = soup.find('table', class_='commonTable')\n",
      "    if result_table:\n",
      "        scientific_names = [italic.text for italic in result_table.find_all('i')]\n",
      "        return scientific_names\n",
      "    else:\n",
      "        return ['No results.']\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The FishBase search url\n",
      "result_url = 'http://fishbase.se/ComNames/CommonNameSearchList.php'\n",
      "# The common name we are serching on\n",
      "common_name = 'Tuna'\n",
      "scientific_names = []\n",
      "next_link = {'href': '?CommonName=' + common_name}\n",
      "# Keep checking for the next link, when it no longer appers in the results we have\n",
      "# them all\n",
      "while next_link:\n",
      "    # Concatenate parts of the url together and have open the url\n",
      "    soup = BeautifulSoup(urllib2.urlopen(result_url + next_link['href']))\n",
      "    scientific_names += page_results(soup)\n",
      "    # If there is an <a href=''>Next</a> tag in the results it means we don't have all\n",
      "    # the results and we should also parse additional results from the next page\n",
      "    next_link = soup.find('a', text='Next')\n",
      "print scientific_names"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'Cybiosarda elegans', u'Thunnus thynnus', u'Thunnus alalunga', u'Thunnus alalunga', u'Thunnus thynnus', u'Katsuwonus pelamis', u'Thunnus alalunga', u'Thunnus albacares', u'Anguilla marmorata', u'Anguilla obscura', u'Anguilla megastoma', u'Anguilla marmorata', u'Euthynnus affinis', u'Gymnosarda unicolor', u'Gymnosarda unicolor', u'Katsuwonus pelamis', u'Thunnus alalunga', u'Thunnus albacares', u'Thunnus albacares', u'Thunnus obesus', u'Thunnus tonggol', u'Muraenesox cinereus', u'Anguilla bengalensis bengalensis', u'Anguilla bicolor bicolor', u'Anguilla marmorata', u'Gymnothorax undulatus', u'Muraenesox bagio', u'Pisodonophis cancrivorus', u'Anguilla nebulosa', u'Congresox talabon', u'Congresox talabonoides', u'Allothunnus fallai', u'Allothunnus fallai', u'Katsuwonus pelamis', u'Katsuwonus pelamis', u'Thunnus alalunga', u'Thunnus alalunga', u'Thunnus albacares', u'Thunnus albacares', u'Thunnus obesus', u'Thunnus obesus', u'Katsuwonus pelamis', u'Thunnus obesus', u'Thunnus obesus', u'Thunnus obesus', u'Anguilla australis australis', u'Anguilla marmorata', u'Anguilla celebesensis', u'Thunnus alalunga', u'Thunnus thynnus']\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}